{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRItspdmCjyXG1ZZiEm5cS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amit-sheikh/Amit-sheikh/blob/main/ml_dl_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# üß† Fake News Detection ‚Äì Comparative Pipeline (5 Models)\n",
        "# Models: Logistic Regression | Naive Bayes | LSTM | RoBERTa | BERT\n",
        "# ===============================================================\n",
        "\n",
        "!pip install transformers datasets torch scikit-learn pandas numpy tqdm --quiet\n",
        "\n",
        "import zipfile, os, numpy as np, pandas as pd, torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# === 1Ô∏è‚É£ Extract Dataset ===\n",
        "zip_path = \"News-_dataset.zip\"\n",
        "extract_dir = \"news_dataset_extracted\"\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"‚úÖ Dataset extracted!\")\n",
        "\n",
        "# === 2Ô∏è‚É£ Load CSVs ===\n",
        "fake = pd.read_csv(os.path.join(extract_dir, \"Fake.csv\"), on_bad_lines='skip', encoding='utf-8')\n",
        "true = pd.read_csv(os.path.join(extract_dir, \"True.csv\"), on_bad_lines='skip', encoding='utf-8')\n",
        "\n",
        "def find_text_col(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == object and len(str(df[col].iloc[0])) > 20:\n",
        "            return col\n",
        "    return df.columns[0]\n",
        "\n",
        "fake_col = find_text_col(fake)\n",
        "true_col = find_text_col(true)\n",
        "\n",
        "fake_df = pd.DataFrame({'text': fake[fake_col].astype(str), 'label': 0})\n",
        "true_df = pd.DataFrame({'text': true[true_col].astype(str), 'label': 1})\n",
        "df = pd.concat([fake_df, true_df], ignore_index=True).sample(frac=1, random_state=42)\n",
        "\n",
        "print(f\"üìä Total samples: {len(df)}\")\n",
        "\n",
        "# === Split ===\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
        ")\n",
        "\n",
        "# ===============================================================\n",
        "# 1Ô∏è‚É£ Logistic Regression (TF-IDF)\n",
        "# ===============================================================\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, train_labels)\n",
        "pred_lr = lr.predict(X_test)\n",
        "acc_lr = accuracy_score(test_labels, pred_lr)\n",
        "print(f\"üîπ Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 2Ô∏è‚É£ Naive Bayes\n",
        "# ===============================================================\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, train_labels)\n",
        "pred_nb = nb.predict(X_test)\n",
        "acc_nb = accuracy_score(test_labels, pred_nb)\n",
        "print(f\"üîπ Naive Bayes Accuracy: {acc_nb:.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 3Ô∏è‚É£ LSTM (Deep Learning)\n",
        "# ===============================================================\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_len)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_len)\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    Embedding(max_words, 128, input_length=max_len),\n",
        "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X_train_seq, np.array(train_labels), epochs=2, batch_size=64, validation_split=0.2, verbose=1)\n",
        "loss, acc_lstm = model_lstm.evaluate(X_test_seq, np.array(test_labels), verbose=0)\n",
        "print(f\"üîπ LSTM Accuracy: {acc_lstm:.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 4Ô∏è‚É£ RoBERTa-base (Transformers Fine-Tuning)\n",
        "# ===============================================================\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_encodings = tokenizer_roberta(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer_roberta(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels.iloc[idx]))\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "args_roberta = TrainingArguments(\n",
        "    output_dir='./roberta_results', evaluation_strategy=\"epoch\", num_train_epochs=1,\n",
        "    per_device_train_batch_size=8, per_device_eval_batch_size=8, learning_rate=2e-5\n",
        ")\n",
        "\n",
        "trainer_roberta = Trainer(\n",
        "    model=model_roberta, args=args_roberta,\n",
        "    train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer_roberta\n",
        ")\n",
        "trainer_roberta.train()\n",
        "metrics_roberta = trainer_roberta.evaluate()\n",
        "acc_roberta = metrics_roberta.get(\"eval_accuracy\", None)\n",
        "print(f\"üîπ RoBERTa-base Accuracy: {acc_roberta:.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 5Ô∏è‚É£ BERT-base (Transformers Fine-Tuning)\n",
        "# ===============================================================\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer_bert(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer_bert(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "test_dataset = NewsDataset(test_encodings, test_labels)\n",
        "\n",
        "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "args_bert = TrainingArguments(\n",
        "    output_dir='./bert_results', evaluation_strategy=\"epoch\", num_train_epochs=1,\n",
        "    per_device_train_batch_size=8, per_device_eval_batch_size=8, learning_rate=2e-5\n",
        ")\n",
        "\n",
        "trainer_bert = Trainer(\n",
        "    model=model_bert, args=args_bert,\n",
        "    train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer_bert\n",
        ")\n",
        "trainer_bert.train()\n",
        "metrics_bert = trainer_bert.evaluate()\n",
        "acc_bert = metrics_bert.get(\"eval_accuracy\", None)\n",
        "print(f\"üîπ BERT-base Accuracy: {acc_bert:.4f}\")\n",
        "\n",
        "# ===============================================================\n",
        "# üìä Summary Table\n",
        "# ===============================================================\n",
        "results = {\n",
        "    \"Logistic Regression\": acc_lr,\n",
        "    \"Naive Bayes\": acc_nb,\n",
        "    \"LSTM\": acc_lstm,\n",
        "    \"RoBERTa-base\": acc_roberta,\n",
        "    \"BERT-base\": acc_bert\n",
        "}\n",
        "\n",
        "print(\"\\nüìà Model Performance Summary:\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:20s}: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "LBLsmJcI6QiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}